{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d34259ba-80ac-4f68-ae9a-9423ecac80bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '20'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import seaborn as sns\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c98183-e171-4c9c-a1e0-abf94eb2cac1",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbe5df79-ac77-41ec-8a5e-2ad41c01f9af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Toy Story (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jumanji (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9737</th>\n",
       "      <td>Black Butler: Book of the Atlantic (2017)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9738</th>\n",
       "      <td>No Game No Life: Zero (2017)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9739</th>\n",
       "      <td>Flint (2017)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9740</th>\n",
       "      <td>Bungo Stray Dogs: Dead Apple (2018)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9741</th>\n",
       "      <td>Andrew Dice Clay: Dice Rules (1991)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9742 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           title\n",
       "index                                           \n",
       "0                               Toy Story (1995)\n",
       "1                                 Jumanji (1995)\n",
       "2                        Grumpier Old Men (1995)\n",
       "3                       Waiting to Exhale (1995)\n",
       "4             Father of the Bride Part II (1995)\n",
       "...                                          ...\n",
       "9737   Black Butler: Book of the Atlantic (2017)\n",
       "9738                No Game No Life: Zero (2017)\n",
       "9739                                Flint (2017)\n",
       "9740         Bungo Stray Dogs: Dead Apple (2018)\n",
       "9741         Andrew Dice Clay: Dice Rules (1991)\n",
       "\n",
       "[9742 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df = pd.read_csv('./temp/ml-latest-small/movies.csv')\n",
    "# In collaborative filtering method, we imagine there is no information about the movies like their categories\n",
    "movies_df = movies_df.drop('genres', axis=1) \n",
    "movies_df.index.name = 'index'\n",
    "# helper dictionaries to help memory usage for embeddings\n",
    "index_to_movieId = dict(movies_df.movieId)\n",
    "movieId_to_index = {v:k for k, v in index_to_movieId.items()}\n",
    "movies_df = movies_df.drop('movieId', axis=1)\n",
    "movies_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32212f2e-f697-4a6c-935d-420e49cfbe61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9434</th>\n",
       "      <td>610</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9461</th>\n",
       "      <td>610</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9462</th>\n",
       "      <td>610</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9463</th>\n",
       "      <td>610</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9503</th>\n",
       "      <td>610</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100836 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       userId  rating\n",
       "index                \n",
       "0           1     4.0\n",
       "2           1     4.0\n",
       "5           1     4.0\n",
       "43          1     5.0\n",
       "46          1     5.0\n",
       "...       ...     ...\n",
       "9434      610     4.0\n",
       "9461      610     5.0\n",
       "9462      610     5.0\n",
       "9463      610     5.0\n",
       "9503      610     3.0\n",
       "\n",
       "[100836 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_df = pd.read_csv('./temp/ml-latest-small/ratings.csv')\n",
    "users_df = users_df.drop('timestamp', axis=1)\n",
    "users_df['index'] = users_df['movieId'].map(lambda x: movieId_to_index[x])\n",
    "\n",
    "# Take the average rating for mean normalization\n",
    "average_rating = users_df.drop('userId', axis=1).groupby('movieId').mean()\n",
    "\n",
    "users_df = users_df.set_index('index')\n",
    "users_df = users_df.drop('movieId', axis=1)\n",
    "\n",
    "users_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8808885-9297-436c-aad5-52d587013230",
   "metadata": {},
   "source": [
    "### Create Movie/User rating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c62b37a5-d629-44b5-a7e8-518c116ea237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>userId</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>601</th>\n",
       "      <th>602</th>\n",
       "      <th>603</th>\n",
       "      <th>604</th>\n",
       "      <th>605</th>\n",
       "      <th>606</th>\n",
       "      <th>607</th>\n",
       "      <th>608</th>\n",
       "      <th>609</th>\n",
       "      <th>610</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9737</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9738</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9739</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9740</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9741</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9724 rows × 610 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "userId  1    2    3    4    5    6    7    8    9    10   ...  601  602  603  \\\n",
       "index                                                     ...                  \n",
       "0       4.0  0.0  0.0  0.0  4.0  0.0  4.5  0.0  0.0  0.0  ...  4.0  0.0  4.0   \n",
       "1       0.0  0.0  0.0  0.0  0.0  4.0  0.0  4.0  0.0  0.0  ...  0.0  4.0  0.0   \n",
       "2       4.0  0.0  0.0  0.0  0.0  5.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "3       0.0  0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "4       0.0  0.0  0.0  0.0  0.0  5.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "...     ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "9737    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "9738    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "9739    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "9740    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "9741    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "\n",
       "userId  604  605  606  607  608  609  610  \n",
       "index                                      \n",
       "0       3.0  4.0  2.5  4.0  2.5  3.0  5.0  \n",
       "1       5.0  3.5  0.0  0.0  2.0  0.0  0.0  \n",
       "2       0.0  0.0  0.0  0.0  2.0  0.0  0.0  \n",
       "3       0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4       3.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "...     ...  ...  ...  ...  ...  ...  ...  \n",
       "9737    0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "9738    0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "9739    0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "9740    0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "9741    0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[9724 rows x 610 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_rating_df = users_df.join(movies_df, on='index')\n",
    "# save the titles mapping, movieId to title, for evaluation\n",
    "titles = movie_rating_df['title']\n",
    "movie_rating_df = movie_rating_df.drop('title', axis=1)\n",
    "movie_rating_df.pivot_table(values='rating', index='index', columns='userId').fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84eb1f26-ea5f-46b4-8a23-776c767d216d",
   "metadata": {},
   "source": [
    "### Getting data ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a977fa7d-3d44-425a-80e4-7f492609a6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training movies has the shape (80668,)\n",
      "Training users has the shape (80668,)\n",
      "Training ratings has the shape (80668,)\n",
      "\n",
      "Test movies has the shape (20168,)\n",
      "Test users has the shape (20168,)\n",
      "Test ratings has the shape (20168,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "movies = movie_rating_df.index.values\n",
    "users = movie_rating_df['userId'].values\n",
    "Y = movie_rating_df['rating'].values.astype('float32')\n",
    "\n",
    "\n",
    "# mean normalize the labels\n",
    "mu = np.mean(Y, axis=0)\n",
    "Y = Y - mu\n",
    "\n",
    "\n",
    "# split the data into training set and validation set\n",
    "training_movies, test_movies, training_users, test_users, training_ratings, test_ratings = train_test_split(movies,\n",
    "                                                                                                            users, \n",
    "                                                                                                            Y,\n",
    "                                                                                                            test_size=0.2, shuffle=True)\n",
    "print(f'Training movies has the shape', training_movies.shape)\n",
    "print(f'Training users has the shape', training_users.shape)\n",
    "print(f'Training ratings has the shape', training_ratings.shape)\n",
    "print()\n",
    "print(f'Test movies has the shape', test_movies.shape)\n",
    "print(f'Test users has the shape', test_users.shape)\n",
    "print(f'Test ratings has the shape', test_ratings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978deb29-dd85-46ba-90e3-c00d8a8c89b3",
   "metadata": {},
   "source": [
    "### Create the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "316ce688-3071-4ec8-a558-13ad67a65c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "BUFFER = 1000\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((training_movies, training_users, training_ratings))\n",
    "train_ds = train_ds.shuffle(BUFFER)\n",
    "train_ds = train_ds.batch(BATCH_SIZE, num_parallel_calls=AUTOTUNE)\n",
    "train_ds = train_ds.prefetch(AUTOTUNE)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_movies, test_users, test_ratings))\n",
    "test_ds = test_ds.batch(BATCH_SIZE, num_parallel_calls=AUTOTUNE)\n",
    "test_ds = test_ds.prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7199a373-e1c8-4107-a8bc-9a8fdcda1d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[5366  474 6534 3640 8301   31    0 5334  314 9369 4361 2028  994 2091\n",
      " 5166  793 8569 2045 5391    5  540 4425 5263 1224  596 6797 3151 3873\n",
      " 1761  505  404 9552 5277 2514  422 1949 2806 5200 9720 7212  969 5819\n",
      "  239  275 3638 8023 7697  794  704 1623  131 6422  907  504 8841 6310\n",
      " 1807  398  733  418 2635  185 1592  302  722 2326 7230 1044 3207 6954\n",
      " 1284 1102 4618 1493 3001 5021 4791 2953 2214 1979    6 6058 5729  948\n",
      "   31  858 3288  828 6405  257  315 2834 1660 2806 4801 6522 2559  926\n",
      " 6102 9706 2642  778 1070 4551  412  900 3232  337 6349 2067 7166 5897\n",
      "  118  911  192  622 4133 6965  126 1719 1002  907 4923   68  239 3010\n",
      "   47  820 7394 1989  123  939 7248 6040  920 2450  969  898 1592 2976\n",
      " 2226 7302  276 5725 2949 2461 4529  339 1162 2867 5269  224 5943 3562\n",
      " 3674 1460  409  506 7214 6689 3885 5794 2079 4309 8404 3002  515 2028\n",
      " 1224    0 7415 4940 3903 6607 2491  957 6947  964 7609 2122 1778 4644\n",
      " 2916 2656  443 8537 3589 2195  257 8964 5335 5911 1789  483 6331 1076\n",
      " 4096 7199    1  957  792 4159 6209 1881 2547 1960 2810  838 8151 1990\n",
      " 1000 1916 3544 1802 1091 1704 8475 2600 7043 7769  675  186 5350 1763\n",
      " 2708  510 6398 4012  690 4571 1840 7948 2409  680  255 2155 6255  898\n",
      " 5374   55 1284 2031 3572 2320 3461    0 1916  203 2712 3447 6607 2806\n",
      " 1067 1591 1541  385 2944   47  355  901 1487  690  510 4262 1640 4769\n",
      "  184  622 2038 4176 7214 1231 4351 3873  725 4476 1171 4297  385 1000\n",
      " 7078  504 3032 3731  315 7090 2195 2992 4351 3517 5917  398 1123  303\n",
      " 2195  325 6648 3381  273  902  221 1551 2248 5498 2674 1432   31 2020\n",
      "   56  257 2302 2018 1962  375  418 1847  702  658  278 2700  455  790\n",
      " 3141 7033 5392 1810  863 1200 1794  939 7066 6853  337  911  473 9278\n",
      " 6352 5985 1466  922 2514 6903  620 3002 5737  935 4908 7456 4137 1002\n",
      "  314  686 3792   81 6613 2608 3039 2828  514 1957 6923 2982 3622 1916\n",
      "  919  613 1000 7620 2475 8807 6028 1071   15 8013  239  309 5695  217\n",
      " 2048  863  509  334  441  769  504 5265  832 9602  325 3476  509  622\n",
      "   18 6681 2970 3470  325 2286 3638 3574 3141 8347  300 1305 1322 1661\n",
      " 4060 4159 4234 3832 2292 6521  792 3885 5680 7302 2581 1224  142  520\n",
      " 6573 1480 6310 3093 2712  659 3640 6743 1445 6630   73 1005 5334  307\n",
      " 7548 1529 3012  945 3646 1870 5872 3638 1284 7268 2446 2816 6191 6234\n",
      " 1522  297 4098  656 8782 2840  585   26  328  140 8190 2110 7395  902\n",
      "  896 7838 3030 2745 1022  613 4616 2047 7503   93 7385   43 6530  801\n",
      " 8379 2507 1243 6008 6310  620 6315 4019 8063  314  694 1987 5695 3439\n",
      "  898 8296  970 2579 2355 2761 8433 1529 1671 3051 7043 2534 5850  615\n",
      " 4133 1803 3568  964 3819 4536 1121  153], shape=(512,), dtype=int64)\n",
      "tf.Tensor(\n",
      "[300 597 111 140 249 513 186 326 477 414 438  45 600 387 190 279 318  41\n",
      " 200 220 136 341 380 261 434 332 219 325 275 512 160 414 474 409 217 597\n",
      " 328 448 248  21 477  89 474  81 137 599  50 313 221 202 274 226 387 173\n",
      " 599 375  68 561 140 517 202 181 414 284 410  91  18 304 309 318  98 438\n",
      " 599 448 414 187 448 450 359 155 179 232 448 275 132 610 610 370 104 560\n",
      " 181  68 177 115 177 381 610 513 599 514 232 517 590 274  11 522 348 385\n",
      " 105 334 567 232 597 534  99 280 414 390 458 580 257 215 509 559 604 387\n",
      " 603 587 274 590  72 303 563 606 328  91 254  52 520 211 495 523  58  18\n",
      " 489 558  50 382 555  57 596 219 483 608 387 474 217  86 414 298 534  52\n",
      " 398 307 380 280 140 115 322 469 298 495 542 105 606 381 332 283  89 420\n",
      " 606 357 221  68 229 317 177 268 212 233 489 525 606 484 387 555 105  89\n",
      " 484  83 525 560 166 575 608 245 280 438 393 499 304 489 344 469 357  18\n",
      " 561 113 424 318 607 181 462 387 288 367 232 580  91 580 236 448 414   4\n",
      " 429 474 274   1 606 389  52 274 479 288  68 121 590 411 380 453 610 474\n",
      " 452 562 387 380 380 156 130 595 305 606 485 381 415 246 103 570 603 110\n",
      " 298 201 356 573 128 474 607 249 134  31 381  83 453 414  32 601 370 216\n",
      " 490 599 356 244 599  58 538 602 307 414 385  75 584 217 597 221 601  70\n",
      "  80 135 608 470 428 383 384 434 415 571 177  45 191 316 373 477 390  62\n",
      " 482 448 453 368 131 376 606 506 177 275   6 448  89 232 307 294 216  98\n",
      " 157 255  68  70 505 232 381 517 177 309 474 606 332 217 195 249 592 125\n",
      " 414 216 346 198 414  33 232 534 101 448 390 294 182 222  84 429 387 447\n",
      " 603 425 605 318 603  95 509 586  51  50  82 438 610 160 249 448 438 256\n",
      " 404 525 534 226 387 323 294 391 534 219 414 119  21 312 305 354 391  68\n",
      " 380 606 448 480 448 266 371  57 599 448 369 596 210 332 100 414 199 382\n",
      " 560 379 318 186 414 525 419 591 542 212  66  68  68  42 274 534 316 603\n",
      " 480 608 448 453 167 337 174  99  50 597 177 220 166 448 111  16  20 553\n",
      " 504 414 448 404 534 141  63 244 610 599 135 387 610 387  68 182 233 423\n",
      "  84 371 105 387 464 279 600 288 580 465 563 414 225  36 247  42 227 226\n",
      " 489 597 232 590 590 594  42 136], shape=(512,), dtype=int64)\n",
      "tf.Tensor(\n",
      "[-5.0155687e-01  4.9844313e-01 -5.0155687e-01  1.4984431e+00\n",
      "  9.9844313e-01  4.9844313e-01  4.9844313e-01  1.4984431e+00\n",
      "  1.4984431e+00  9.9844313e-01  4.9844313e-01  1.4984431e+00\n",
      " -2.0015569e+00 -1.5568733e-03  1.4984431e+00  1.4984431e+00\n",
      " -1.5568733e-03 -1.5015569e+00 -1.0015569e+00 -1.5568733e-03\n",
      "  4.9844313e-01  9.9844313e-01 -1.5568733e-03 -1.5568733e-03\n",
      "  1.4984431e+00  4.9844313e-01 -5.0155687e-01  4.9844313e-01\n",
      " -1.5015569e+00  4.9844313e-01  4.9844313e-01  4.9844313e-01\n",
      " -1.0015569e+00  1.4984431e+00 -5.0155687e-01  4.9844313e-01\n",
      " -5.0155687e-01 -1.5568733e-03 -1.5568733e-03  4.9844313e-01\n",
      "  9.9844313e-01  1.4984431e+00 -1.5015569e+00  4.9844313e-01\n",
      "  1.4984431e+00 -5.0155687e-01 -1.0015569e+00  4.9844313e-01\n",
      "  1.4984431e+00 -5.0155687e-01 -1.5568733e-03  9.9844313e-01\n",
      "  1.4984431e+00 -5.0155687e-01 -1.5568733e-03  1.4984431e+00\n",
      " -5.0155687e-01  4.9844313e-01 -5.0155687e-01 -2.5015569e+00\n",
      "  4.9844313e-01 -1.5015569e+00  1.4984431e+00  1.4984431e+00\n",
      "  1.4984431e+00  1.4984431e+00 -1.5568733e-03  1.4984431e+00\n",
      "  4.9844313e-01 -1.5568733e-03  4.9844313e-01 -5.0155687e-01\n",
      " -1.5568733e-03  4.9844313e-01  4.9844313e-01  4.9844313e-01\n",
      "  4.9844313e-01  4.9844313e-01 -1.5568733e-03  1.4984431e+00\n",
      " -5.0155687e-01 -1.5568733e-03 -1.5568733e-03  1.4984431e+00\n",
      " -5.0155687e-01  4.9844313e-01 -1.5568733e-03 -1.0015569e+00\n",
      " -1.5568733e-03  4.9844313e-01 -2.5015569e+00  4.9844313e-01\n",
      "  4.9844313e-01 -5.0155687e-01 -5.0155687e-01  9.9844313e-01\n",
      "  9.9844313e-01  1.4984431e+00 -5.0155687e-01 -5.0155687e-01\n",
      " -1.5568733e-03 -1.5015569e+00 -5.0155687e-01 -2.5015569e+00\n",
      "  4.9844313e-01 -1.5568733e-03 -1.5568733e-03 -5.0155687e-01\n",
      "  4.9844313e-01 -1.5568733e-03  1.4984431e+00 -5.0155687e-01\n",
      " -1.5015569e+00  9.9844313e-01  1.4984431e+00 -1.0015569e+00\n",
      "  4.9844313e-01  4.9844313e-01  1.4984431e+00 -5.0155687e-01\n",
      " -1.0015569e+00  4.9844313e-01 -5.0155687e-01 -5.0155687e-01\n",
      "  4.9844313e-01  4.9844313e-01 -2.5015569e+00  4.9844313e-01\n",
      " -5.0155687e-01 -1.5015569e+00  4.9844313e-01  1.4984431e+00\n",
      " -5.0155687e-01 -5.0155687e-01  1.4984431e+00 -1.5015569e+00\n",
      " -1.5568733e-03  4.9844313e-01 -1.5568733e-03 -1.5568733e-03\n",
      "  1.4984431e+00  1.4984431e+00 -5.0155687e-01  9.9844313e-01\n",
      " -2.0015569e+00  4.9844313e-01 -5.0155687e-01 -1.5568733e-03\n",
      "  4.9844313e-01 -1.5015569e+00 -1.5568733e-03  1.4984431e+00\n",
      " -1.0015569e+00  9.9844313e-01 -5.0155687e-01 -1.5015569e+00\n",
      " -1.5015569e+00  4.9844313e-01  4.9844313e-01 -2.0015569e+00\n",
      " -1.5568733e-03 -5.0155687e-01  4.9844313e-01 -1.0015569e+00\n",
      "  4.9844313e-01 -5.0155687e-01  4.9844313e-01  4.9844313e-01\n",
      "  1.4984431e+00  4.9844313e-01 -1.5568733e-03  9.9844313e-01\n",
      " -3.0015569e+00  4.9844313e-01 -1.5568733e-03  4.9844313e-01\n",
      " -1.5568733e-03 -5.0155687e-01 -1.0015569e+00  4.9844313e-01\n",
      " -1.0015569e+00  9.9844313e-01  1.4984431e+00 -1.5568733e-03\n",
      "  4.9844313e-01  4.9844313e-01  9.9844313e-01 -5.0155687e-01\n",
      " -1.5568733e-03 -1.5015569e+00  9.9844313e-01  9.9844313e-01\n",
      "  4.9844313e-01  4.9844313e-01  9.9844313e-01 -5.0155687e-01\n",
      "  4.9844313e-01  1.4984431e+00 -1.0015569e+00  1.4984431e+00\n",
      "  9.9844313e-01  4.9844313e-01  9.9844313e-01 -2.5015569e+00\n",
      " -5.0155687e-01 -5.0155687e-01  9.9844313e-01  4.9844313e-01\n",
      "  4.9844313e-01 -1.0015569e+00  4.9844313e-01 -1.5015569e+00\n",
      " -2.0015569e+00 -1.5015569e+00  9.9844313e-01  4.9844313e-01\n",
      "  4.9844313e-01  4.9844313e-01 -1.5568733e-03 -5.0155687e-01\n",
      "  1.4984431e+00 -5.0155687e-01  9.9844313e-01  4.9844313e-01\n",
      " -5.0155687e-01  1.4984431e+00 -2.0015569e+00  4.9844313e-01\n",
      " -5.0155687e-01  9.9844313e-01  1.4984431e+00 -2.0015569e+00\n",
      "  4.9844313e-01  1.4984431e+00 -5.0155687e-01 -5.0155687e-01\n",
      " -2.0015569e+00  1.4984431e+00  4.9844313e-01  1.4984431e+00\n",
      "  1.4984431e+00 -1.5568733e-03 -5.0155687e-01 -5.0155687e-01\n",
      " -5.0155687e-01  4.9844313e-01 -1.5015569e+00 -5.0155687e-01\n",
      "  4.9844313e-01  1.4984431e+00 -1.5568733e-03 -1.5568733e-03\n",
      "  1.4984431e+00  9.9844313e-01  9.9844313e-01  1.4984431e+00\n",
      " -5.0155687e-01 -1.5568733e-03  4.9844313e-01  1.4984431e+00\n",
      "  4.9844313e-01  9.9844313e-01  1.4984431e+00  9.9844313e-01\n",
      "  4.9844313e-01  9.9844313e-01  4.9844313e-01 -1.5015569e+00\n",
      " -5.0155687e-01  9.9844313e-01 -5.0155687e-01  1.4984431e+00\n",
      "  4.9844313e-01  9.9844313e-01  1.4984431e+00 -1.0015569e+00\n",
      "  1.4984431e+00  4.9844313e-01  4.9844313e-01  4.9844313e-01\n",
      "  1.4984431e+00 -2.0015569e+00 -2.5015569e+00 -5.0155687e-01\n",
      " -5.0155687e-01  9.9844313e-01 -1.5568733e-03 -1.5015569e+00\n",
      " -1.0015569e+00 -1.0015569e+00  1.4984431e+00  1.4984431e+00\n",
      " -1.5015569e+00  4.9844313e-01  9.9844313e-01  1.4984431e+00\n",
      "  4.9844313e-01 -1.5015569e+00 -5.0155687e-01 -1.0015569e+00\n",
      "  1.4984431e+00 -5.0155687e-01  4.9844313e-01  4.9844313e-01\n",
      "  9.9844313e-01 -1.5568733e-03  4.9844313e-01  4.9844313e-01\n",
      " -3.0015569e+00 -5.0155687e-01  9.9844313e-01  4.9844313e-01\n",
      " -5.0155687e-01 -1.5568733e-03  4.9844313e-01  4.9844313e-01\n",
      " -1.5568733e-03  4.9844313e-01  4.9844313e-01 -2.0015569e+00\n",
      " -2.5015569e+00 -1.5568733e-03  9.9844313e-01  4.9844313e-01\n",
      "  9.9844313e-01 -5.0155687e-01  1.4984431e+00 -1.5015569e+00\n",
      " -5.0155687e-01  4.9844313e-01 -5.0155687e-01 -2.5015569e+00\n",
      " -5.0155687e-01  4.9844313e-01 -5.0155687e-01 -2.0015569e+00\n",
      " -1.0015569e+00 -1.5568733e-03 -5.0155687e-01  4.9844313e-01\n",
      "  4.9844313e-01 -5.0155687e-01  1.4984431e+00 -1.5015569e+00\n",
      " -1.0015569e+00  4.9844313e-01  1.4984431e+00  1.4984431e+00\n",
      "  4.9844313e-01 -1.5015569e+00  1.4984431e+00  4.9844313e-01\n",
      " -1.5015569e+00 -1.5568733e-03  4.9844313e-01 -1.5015569e+00\n",
      "  4.9844313e-01 -1.5568733e-03 -5.0155687e-01  9.9844313e-01\n",
      "  4.9844313e-01  1.4984431e+00 -5.0155687e-01 -5.0155687e-01\n",
      "  4.9844313e-01  4.9844313e-01 -1.5568733e-03 -1.0015569e+00\n",
      "  4.9844313e-01 -5.0155687e-01 -1.0015569e+00 -5.0155687e-01\n",
      "  1.4984431e+00 -5.0155687e-01 -5.0155687e-01  4.9844313e-01\n",
      "  1.4984431e+00  4.9844313e-01 -5.0155687e-01 -1.5568733e-03\n",
      " -5.0155687e-01 -1.5568733e-03  1.4984431e+00  4.9844313e-01\n",
      " -2.5015569e+00 -5.0155687e-01  1.4984431e+00 -2.5015569e+00\n",
      " -1.5568733e-03 -1.5568733e-03  9.9844313e-01 -2.5015569e+00\n",
      " -1.5568733e-03 -2.0015569e+00 -5.0155687e-01 -1.5568733e-03\n",
      " -5.0155687e-01  4.9844313e-01  4.9844313e-01 -1.5568733e-03\n",
      "  9.9844313e-01 -1.0015569e+00 -2.5015569e+00  1.4984431e+00\n",
      " -1.5568733e-03  4.9844313e-01  4.9844313e-01  9.9844313e-01\n",
      " -1.5015569e+00  4.9844313e-01  1.4984431e+00  9.9844313e-01\n",
      "  1.4984431e+00 -1.5568733e-03  4.9844313e-01  4.9844313e-01\n",
      "  4.9844313e-01 -1.5015569e+00 -5.0155687e-01  1.4984431e+00\n",
      "  1.4984431e+00 -5.0155687e-01 -5.0155687e-01 -1.5015569e+00\n",
      " -5.0155687e-01 -1.5568733e-03  9.9844313e-01 -1.5568733e-03\n",
      "  9.9844313e-01 -1.5568733e-03 -5.0155687e-01  9.9844313e-01\n",
      "  4.9844313e-01  4.9844313e-01  4.9844313e-01  4.9844313e-01\n",
      " -1.5015569e+00  9.9844313e-01  4.9844313e-01 -5.0155687e-01\n",
      "  4.9844313e-01 -5.0155687e-01  1.4984431e+00  4.9844313e-01\n",
      " -1.0015569e+00 -1.5015569e+00 -1.5568733e-03 -5.0155687e-01\n",
      " -1.5015569e+00 -5.0155687e-01 -2.5015569e+00 -5.0155687e-01\n",
      " -3.0015569e+00  4.9844313e-01  4.9844313e-01  1.4984431e+00\n",
      " -5.0155687e-01  1.4984431e+00 -2.5015569e+00  1.4984431e+00\n",
      "  1.4984431e+00  1.4984431e+00  4.9844313e-01 -5.0155687e-01\n",
      " -1.0015569e+00  4.9844313e-01 -1.5568733e-03  1.4984431e+00\n",
      "  4.9844313e-01  4.9844313e-01 -1.5015569e+00 -5.0155687e-01\n",
      " -1.5568733e-03 -5.0155687e-01  4.9844313e-01  4.9844313e-01\n",
      " -1.5568733e-03 -2.0015569e+00  1.4984431e+00 -1.5568733e-03\n",
      "  9.9844313e-01 -1.0015569e+00 -1.5568733e-03 -5.0155687e-01\n",
      " -1.5568733e-03  1.4984431e+00  4.9844313e-01 -1.5568733e-03\n",
      "  1.4984431e+00 -1.5568733e-03  1.4984431e+00 -1.5015569e+00\n",
      "  4.9844313e-01 -1.5568733e-03 -1.5568733e-03  4.9844313e-01\n",
      " -1.5568733e-03 -5.0155687e-01  4.9844313e-01 -1.5015569e+00\n",
      "  4.9844313e-01 -5.0155687e-01  4.9844313e-01 -5.0155687e-01\n",
      "  4.9844313e-01  1.4984431e+00  4.9844313e-01  4.9844313e-01\n",
      " -5.0155687e-01  4.9844313e-01 -5.0155687e-01 -2.5015569e+00], shape=(512,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for movie, user, rate in train_ds.take(1):\n",
    "    print(movie)\n",
    "    print(user)\n",
    "    print(rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969d4bde-e343-434a-abcc-1a90c2e4e080",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00b02f8a-7ad9-4465-975b-f9ee12d59fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Activation, Dropout, Input, Dot, Embedding, Layer\n",
    "from keras.losses import MeanSquaredError\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import MeanAbsoluteError, Metric\n",
    "from keras.models import Model\n",
    "from keras.regularizers import L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a633433-27fa-4e37-842b-1ee6cc74b308",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollaborativeFiltering(Model):\n",
    "    def __init__(self, \n",
    "                 units,\n",
    "                 num_movies, \n",
    "                 num_users, \n",
    "                 user_embedding_dim, \n",
    "                 movie_embbeding_dim):\n",
    "        \n",
    "        super(CollaborativeFiltering, self).__init__()\n",
    "        self.user_embedding = Embedding(input_dim=num_users, \n",
    "                                        output_dim=user_embedding_dim, \n",
    "                                        name='user_embed')\n",
    "        self.movie_embedding = Embedding(input_dim=num_movies, \n",
    "                                         output_dim=movie_embbeding_dim, \n",
    "                                         name='movie_embed')\n",
    "\n",
    "        self.user_mapping = Dense(units=units, kernel_regularizer=L2(0.1))\n",
    "        self.movie_mapping = Dense(units=units, kernel_regularizer=L2(0.1))\n",
    "        self.dot = Dot(axes=1, normalize=True, name='dot')\n",
    "\n",
    "        self.build()\n",
    "\n",
    "    \n",
    "    def build(self):\n",
    "        user = Input(shape=())\n",
    "        movie = Input(shape=())\n",
    "        self.call(movie, user)\n",
    "        self.built = True\n",
    "    \n",
    "    def call(self, movie, user, training=False):\n",
    "        user = self.user_embedding(user)\n",
    "        movie = self.movie_embedding(movie)\n",
    "\n",
    "        user = self.user_mapping(user)\n",
    "        movie = self.movie_mapping(movie)\n",
    "        return self.dot([user, movie])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32f9aa61-d694-49be-b0cb-2b96ffb05e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"collaborative_filtering\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " user_embed (Embedding)      (None, 512)               312832    \n",
      "                                                                 \n",
      " movie_embed (Embedding)     (None, 512)               4979200   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " dot (Dot)                   (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5817344 (22.19 MB)\n",
      "Trainable params: 5817344 (22.19 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = CollaborativeFiltering(units=512,\n",
    "                               num_movies=np.unique(movies).shape[0] + 1, \n",
    "                               num_users=np.unique(users).shape[0] + 1, \n",
    "                               user_embedding_dim=512, \n",
    "                               movie_embbeding_dim=512)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47f740b3-8045-4c55-944b-df4a7ebb8147",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = MeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ea3d3bc-041f-44ff-8cd7-91e092c465a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scheduler(initial_learning_rate=1e-3, min_learning_rate=1e-5, weight=0.9):\n",
    "    def func(epoch):\n",
    "        return max(initial_learning_rate * weight ** (epoch), min_learning_rate)\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d5159c3-51d6-4310-9659-95902c0ecd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mae = MeanAbsoluteError(name='train_mae')\n",
    "validation_mae = MeanAbsoluteError(name='test_mae')\n",
    "optimizer = Adam()\n",
    "scheduler = get_scheduler(initial_learning_rate=1e-2, weight=0.93)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08300e3-8104-4aac-b910-7027d3cbb492",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b292e36e-3af4-4f65-ae9a-96fb04e9ef95",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def training_step(movie, user, y_true):\n",
    "    loss = 0.0\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(movie, user, training=True)\n",
    "        loss = loss_func(y_true, y_pred)\n",
    "        train_mae.update_state(y_true, y_pred)\n",
    "        \n",
    "    variables = model.trainable_variables\n",
    "    grads = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(grads, variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d69fbbbb-e9d1-4eac-9e3d-39e58d76efa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def validation_step(movie, user, y_true):\n",
    "    y_pred = model(movie, user, training=False)\n",
    "    loss = loss_func(y_true, y_pred)\n",
    "    validation_mae.update_state(y_true, y_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da66c7bd-c7ea-41f5-96bc-d2d9f58c555b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1715017203.010304   96659 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8 sec | Step 157\tLoss: 0.8541\t MAE: 0.7140\n",
      "  8 sec | Step  39\tLoss: 0.7701\t MAE: 0.6755\n",
      "Epoch   2\n",
      "  1 sec | Step 157\tLoss: 0.6790\t MAE: 0.6300\n",
      "  1 sec | Step  39\tLoss: 0.8078\t MAE: 0.6931\n",
      "Epoch   3\n",
      "  0 sec | Step 157\tLoss: 0.6282\t MAE: 0.6038\n",
      "  0 sec | Step  39\tLoss: 0.8085\t MAE: 0.6924\n",
      "Epoch   4\n",
      "  0 sec | Step 157\tLoss: 0.6062\t MAE: 0.5927\n",
      "  0 sec | Step  39\tLoss: 0.8000\t MAE: 0.6884\n",
      "Epoch   5\n",
      "  0 sec | Step 157\tLoss: 0.5954\t MAE: 0.5859\n",
      "  0 sec | Step  39\tLoss: 0.7913\t MAE: 0.6834\n",
      "Epoch   6\n",
      "  0 sec | Step 157\tLoss: 0.5813\t MAE: 0.5765\n",
      "  0 sec | Step  39\tLoss: 0.7856\t MAE: 0.6805\n",
      "Epoch   7\n",
      "  0 sec | Step 157\tLoss: 0.5695\t MAE: 0.5673\n",
      "  0 sec | Step  39\tLoss: 0.7823\t MAE: 0.6792\n",
      "Epoch   8\n",
      "  0 sec | Step 157\tLoss: 0.5612\t MAE: 0.5614\n",
      "  0 sec | Step  39\tLoss: 0.7804\t MAE: 0.6785\n",
      "Epoch   9\n",
      "  0 sec | Step 157\tLoss: 0.5548\t MAE: 0.5572\n",
      "  0 sec | Step  39\tLoss: 0.7787\t MAE: 0.6778\n",
      "Epoch  10\n",
      "  0 sec | Step 157\tLoss: 0.5508\t MAE: 0.5544\n",
      "  0 sec | Step  39\tLoss: 0.7782\t MAE: 0.6777\n",
      "Epoch  11\n",
      "  0 sec | Step 157\tLoss: 0.5469\t MAE: 0.5519\n",
      "  0 sec | Step  39\tLoss: 0.7777\t MAE: 0.6776\n",
      "Epoch  12\n",
      "  0 sec | Step 157\tLoss: 0.5439\t MAE: 0.5500\n",
      "  0 sec | Step  39\tLoss: 0.7772\t MAE: 0.6775\n",
      "Epoch  13\n",
      "  0 sec | Step 157\tLoss: 0.5412\t MAE: 0.5481\n",
      "  0 sec | Step  39\tLoss: 0.7767\t MAE: 0.6773\n",
      "Epoch  14\n",
      "  0 sec | Step 157\tLoss: 0.5388\t MAE: 0.5466\n",
      "  0 sec | Step  39\tLoss: 0.7763\t MAE: 0.6774\n",
      "Epoch  15\n",
      "  0 sec | Step 157\tLoss: 0.5366\t MAE: 0.5451\n",
      "  0 sec | Step  39\tLoss: 0.7762\t MAE: 0.6774\n",
      "Epoch  16\n",
      "  0 sec | Step 157\tLoss: 0.5350\t MAE: 0.5440\n",
      "  0 sec | Step  39\tLoss: 0.7758\t MAE: 0.6772\n",
      "Epoch  17\n",
      "  0 sec | Step 157\tLoss: 0.5332\t MAE: 0.5429\n",
      "  0 sec | Step  39\tLoss: 0.7760\t MAE: 0.6773\n",
      "Epoch  18\n",
      "  0 sec | Step 157\tLoss: 0.5316\t MAE: 0.5418\n",
      "  0 sec | Step  39\tLoss: 0.7756\t MAE: 0.6772\n",
      "Epoch  19\n",
      "  0 sec | Step 157\tLoss: 0.5302\t MAE: 0.5409\n",
      "  0 sec | Step  39\tLoss: 0.7756\t MAE: 0.6773\n",
      "Epoch  20\n",
      "  0 sec | Step 157\tLoss: 0.5287\t MAE: 0.5400\n",
      "  0 sec | Step  39\tLoss: 0.7754\t MAE: 0.6772\n",
      "Epoch  21\n",
      "  0 sec | Step 157\tLoss: 0.5276\t MAE: 0.5392\n",
      "  0 sec | Step  39\tLoss: 0.7753\t MAE: 0.6772\n",
      "Epoch  22\n",
      "  0 sec | Step 157\tLoss: 0.5265\t MAE: 0.5385\n",
      "  0 sec | Step  39\tLoss: 0.7755\t MAE: 0.6773\n",
      "Epoch  23\n",
      "  0 sec | Step 157\tLoss: 0.5253\t MAE: 0.5377\n",
      "  0 sec | Step  39\tLoss: 0.7755\t MAE: 0.6773\n",
      "Epoch  24\n",
      "  0 sec | Step 157\tLoss: 0.5240\t MAE: 0.5370\n",
      "  0 sec | Step  39\tLoss: 0.7753\t MAE: 0.6772\n",
      "Epoch  25\n",
      "  0 sec | Step 157\tLoss: 0.5232\t MAE: 0.5364\n",
      "  0 sec | Step  39\tLoss: 0.7754\t MAE: 0.6773\n",
      "Epoch  26\n",
      "  0 sec | Step 157\tLoss: 0.5222\t MAE: 0.5358\n",
      "  0 sec | Step  39\tLoss: 0.7754\t MAE: 0.6772\n",
      "Epoch  27\n",
      "  0 sec | Step 157\tLoss: 0.5212\t MAE: 0.5352\n",
      "  0 sec | Step  39\tLoss: 0.7753\t MAE: 0.6772\n",
      "Epoch  28\n",
      "  0 sec | Step 157\tLoss: 0.5204\t MAE: 0.5346\n",
      "  0 sec | Step  39\tLoss: 0.7753\t MAE: 0.6772\n",
      "Epoch  29\n",
      "  0 sec | Step 157\tLoss: 0.5197\t MAE: 0.5341\n",
      "  0 sec | Step  39\tLoss: 0.7754\t MAE: 0.6774\n",
      "Epoch  30\n",
      "  0 sec | Step 157\tLoss: 0.5191\t MAE: 0.5337\n",
      "  0 sec | Step  39\tLoss: 0.7754\t MAE: 0.6773\n",
      "Epoch  31\n",
      "  0 sec | Step 157\tLoss: 0.5186\t MAE: 0.5333\n",
      "  0 sec | Step  39\tLoss: 0.7754\t MAE: 0.6774\n",
      "Epoch  32\n",
      "  0 sec | Step 157\tLoss: 0.5178\t MAE: 0.5329\n",
      "  0 sec | Step  39\tLoss: 0.7754\t MAE: 0.6774\n",
      "Epoch  33\n",
      "  0 sec | Step 157\tLoss: 0.5172\t MAE: 0.5325\n",
      "  0 sec | Step  39\tLoss: 0.7754\t MAE: 0.6774\n",
      "Epoch  34\n",
      "  0 sec | Step 157\tLoss: 0.5168\t MAE: 0.5322\n",
      "  0 sec | Step  39\tLoss: 0.7753\t MAE: 0.6774\n",
      "Epoch  35\n",
      "  0 sec | Step 157\tLoss: 0.5159\t MAE: 0.5317\n",
      "  0 sec | Step  39\tLoss: 0.7754\t MAE: 0.6775\n",
      "Epoch  36\n",
      "  0 sec | Step 157\tLoss: 0.5154\t MAE: 0.5313\n",
      "  0 sec | Step  39\tLoss: 0.7755\t MAE: 0.6775\n",
      "Epoch  37\n",
      "  0 sec | Step 157\tLoss: 0.5153\t MAE: 0.5312\n",
      "  0 sec | Step  39\tLoss: 0.7755\t MAE: 0.6774\n",
      "Epoch  38\n",
      "  0 sec | Step 157\tLoss: 0.5150\t MAE: 0.5309\n",
      "  0 sec | Step  39\tLoss: 0.7754\t MAE: 0.6775\n",
      "Epoch  39\n",
      "  0 sec | Step 157\tLoss: 0.5141\t MAE: 0.5305\n",
      "  0 sec | Step  39\tLoss: 0.7755\t MAE: 0.6774\n",
      "Epoch  40\n",
      "  0 sec | Step 157\tLoss: 0.5138\t MAE: 0.5303\n",
      "  0 sec | Step  39\tLoss: 0.7755\t MAE: 0.6774\n",
      "Epoch  41\n",
      "  0 sec | Step 157\tLoss: 0.5134\t MAE: 0.5300\n",
      "  0 sec | Step  39\tLoss: 0.7756\t MAE: 0.6775\n",
      "Epoch  42\n",
      "  0 sec | Step 157\tLoss: 0.5135\t MAE: 0.5300\n",
      "  0 sec | Step  39\tLoss: 0.7755\t MAE: 0.6775\n",
      "Epoch  43\n",
      "  0 sec | Step 157\tLoss: 0.5129\t MAE: 0.5297\n",
      "  0 sec | Step  39\tLoss: 0.7756\t MAE: 0.6775\n",
      "Epoch  44\n",
      "  0 sec | Step 157\tLoss: 0.5124\t MAE: 0.5294\n",
      "  0 sec | Step  39\tLoss: 0.7755\t MAE: 0.6775\n",
      "Epoch  45\n",
      "  0 sec | Step 157\tLoss: 0.5120\t MAE: 0.5291\n",
      "  0 sec | Step  39\tLoss: 0.7756\t MAE: 0.6775\n",
      "Epoch  46\n",
      "  0 sec | Step 157\tLoss: 0.5119\t MAE: 0.5290\n",
      "  0 sec | Step  39\tLoss: 0.7755\t MAE: 0.6775\n",
      "Epoch  47\n",
      "  0 sec | Step 157\tLoss: 0.5117\t MAE: 0.5289\n",
      "  0 sec | Step  39\tLoss: 0.7756\t MAE: 0.6775\n",
      "Epoch  48\n",
      "  0 sec | Step 157\tLoss: 0.5116\t MAE: 0.5288\n",
      "  0 sec | Step  39\tLoss: 0.7756\t MAE: 0.6775\n",
      "Epoch  49\n",
      "  0 sec | Step 157\tLoss: 0.5115\t MAE: 0.5287\n",
      "  0 sec | Step  39\tLoss: 0.7756\t MAE: 0.6775\n",
      "Epoch  50\n",
      "  0 sec | Step 157\tLoss: 0.5112\t MAE: 0.5285\n",
      "  0 sec | Step  39\tLoss: 0.7756\t MAE: 0.6775\n",
      "Epoch  51\n",
      "  0 sec | Step 157\tLoss: 0.5109\t MAE: 0.5284\n",
      "  0 sec | Step  39\tLoss: 0.7756\t MAE: 0.6775\n",
      "Epoch  52\n",
      "  0 sec | Step 157\tLoss: 0.5109\t MAE: 0.5283\n",
      "  0 sec | Step  39\tLoss: 0.7756\t MAE: 0.6775\n",
      "Epoch  53\n",
      "  0 sec | Step 157\tLoss: 0.5105\t MAE: 0.5282\n",
      "  0 sec | Step  39\tLoss: 0.7757\t MAE: 0.6776\n",
      "Epoch  54\n",
      "  0 sec | Step 157\tLoss: 0.5105\t MAE: 0.5280\n",
      "  0 sec | Step  39\tLoss: 0.7757\t MAE: 0.6776\n",
      "Epoch  55\n",
      "  0 sec | Step 157\tLoss: 0.5103\t MAE: 0.5279\n",
      "  0 sec | Step  39\tLoss: 0.7757\t MAE: 0.6776\n",
      "Epoch  56\n",
      "  0 sec | Step 157\tLoss: 0.5103\t MAE: 0.5279\n",
      "  0 sec | Step  39\tLoss: 0.7757\t MAE: 0.6776\n",
      "Epoch  57\n",
      "  0 sec | Step 157\tLoss: 0.5102\t MAE: 0.5279\n",
      "  0 sec | Step  39\tLoss: 0.7757\t MAE: 0.6776\n",
      "Epoch  58\n",
      "  0 sec | Step 157\tLoss: 0.5099\t MAE: 0.5277\n",
      "  0 sec | Step  39\tLoss: 0.7757\t MAE: 0.6776\n",
      "Epoch  59\n",
      "  0 sec | Step 157\tLoss: 0.5100\t MAE: 0.5278\n",
      "  0 sec | Step  39\tLoss: 0.7757\t MAE: 0.6776\n",
      "Epoch  60\n",
      "  0 sec | Step 157\tLoss: 0.5099\t MAE: 0.5277\n",
      "  0 sec | Step  39\tLoss: 0.7757\t MAE: 0.6776\n",
      "Epoch  61\n",
      "  0 sec | Step 157\tLoss: 0.5097\t MAE: 0.5275\n",
      "  0 sec | Step  39\tLoss: 0.7758\t MAE: 0.6776\n",
      "Epoch  62\n",
      "  0 sec | Step 157\tLoss: 0.5096\t MAE: 0.5274\n",
      "  0 sec | Step  39\tLoss: 0.7758\t MAE: 0.6776\n",
      "Epoch  63\n",
      "  0 sec | Step 157\tLoss: 0.5095\t MAE: 0.5274\n",
      "  0 sec | Step  39\tLoss: 0.7758\t MAE: 0.6776\n",
      "Epoch  64\n",
      "  0 sec | Step 157\tLoss: 0.5094\t MAE: 0.5273\n",
      "  0 sec | Step  39\tLoss: 0.7758\t MAE: 0.6776\n",
      "Epoch  65\n",
      "  0 sec | Step 157\tLoss: 0.5095\t MAE: 0.5274\n",
      "  0 sec | Step  39\tLoss: 0.7758\t MAE: 0.6776\n",
      "Epoch  66\n",
      "  0 sec | Step 157\tLoss: 0.5091\t MAE: 0.5272\n",
      "  0 sec | Step  39\tLoss: 0.7758\t MAE: 0.6776\n",
      "Epoch  67\n",
      "  0 sec | Step 157\tLoss: 0.5091\t MAE: 0.5272\n",
      "  0 sec | Step  39\tLoss: 0.7758\t MAE: 0.6776\n",
      "Epoch  68\n",
      "  0 sec | Step 157\tLoss: 0.5094\t MAE: 0.5273\n",
      "  0 sec | Step  39\tLoss: 0.7758\t MAE: 0.6776\n",
      "Epoch  69\n",
      "  0 sec | Step 157\tLoss: 0.5091\t MAE: 0.5271\n",
      "  0 sec | Step  39\tLoss: 0.7758\t MAE: 0.6776\n",
      "Epoch  70\n",
      "  0 sec | Step 157\tLoss: 0.5092\t MAE: 0.5273\n",
      "  0 sec | Step  39\tLoss: 0.7758\t MAE: 0.6776\n",
      "Epoch  71\n",
      "  0 sec | Step 157\tLoss: 0.5089\t MAE: 0.5270\n",
      "  0 sec | Step  39\tLoss: 0.7758\t MAE: 0.6776\n",
      "Epoch  72\n",
      "  0 sec | Step 157\tLoss: 0.5088\t MAE: 0.5269\n",
      "  0 sec | Step  39\tLoss: 0.7758\t MAE: 0.6776\n",
      "Epoch  73\n",
      "  0 sec | Step 157\tLoss: 0.5090\t MAE: 0.5271\n",
      "  0 sec | Step  39\tLoss: 0.7758\t MAE: 0.6776\n",
      "Epoch  74\n",
      "  0 sec | Step 157\tLoss: 0.5087\t MAE: 0.5269\n",
      "  0 sec | Step  39\tLoss: 0.7758\t MAE: 0.6776\n",
      "Epoch  75\n",
      "  0 sec | Step 157\tLoss: 0.5087\t MAE: 0.5270\n",
      "  0 sec | Step  39\tLoss: 0.7758\t MAE: 0.6776\n",
      "Epoch  76\n",
      "  0 sec | Step 157\tLoss: 0.5090\t MAE: 0.5271\n",
      "  0 sec | Step  39\tLoss: 0.7758\t MAE: 0.6776\n",
      "Epoch  77\n",
      "  0 sec | Step 157\tLoss: 0.5089\t MAE: 0.5270\n",
      "  0 sec | Step  39\tLoss: 0.7758\t MAE: 0.6776\n",
      "Epoch  78\n",
      "  0 sec | Step 157\tLoss: 0.5088\t MAE: 0.5270\n",
      "  0 sec | Step  39\tLoss: 0.7758\t MAE: 0.6776\n",
      "Epoch  79\n",
      "  0 sec | Step 157\tLoss: 0.5089\t MAE: 0.5270\n",
      "  0 sec | Step  39\tLoss: 0.7758\t MAE: 0.6776\n",
      "Epoch  80\n",
      "  0 sec | Step 157\tLoss: 0.5088\t MAE: 0.5270\n",
      "  0 sec | Step  39\tLoss: 0.7758\t MAE: 0.6776\n",
      "Epoch  81\n",
      "  0 sec | Step 157\tLoss: 0.5089\t MAE: 0.5269\n",
      "  0 sec | Step  39\tLoss: 0.7758\t MAE: 0.6776\n",
      "Epoch  82\n",
      "  0 sec | Step 157\tLoss: 0.5088\t MAE: 0.5269\n",
      "  0 sec | Step  39\tLoss: 0.7758\t MAE: 0.6776\n",
      "Epoch  83\n",
      "  0 sec | Step 157\tLoss: 0.5088\t MAE: 0.5270\n",
      "  0 sec | Step  39\tLoss: 0.7758\t MAE: 0.6776\n",
      "Epoch  84\n",
      "  0 sec | Step 157\tLoss: 0.5086\t MAE: 0.5268\n",
      "  0 sec | Step  39\tLoss: 0.7758\t MAE: 0.6776\n",
      "Epoch  85\n",
      "  0 sec | Step 157\tLoss: 0.5089\t MAE: 0.5269\n",
      "  0 sec | Step  39\tLoss: 0.7758\t MAE: 0.6776\n",
      "Epoch  86\n",
      "  0 sec | Step 157\tLoss: 0.5086\t MAE: 0.5268\n",
      "  0 sec | Step  39\tLoss: 0.7758\t MAE: 0.6776\n",
      "Epoch  87\n",
      "  0 sec | Step 157\tLoss: 0.5086\t MAE: 0.5268\n",
      "  0 sec | Step  39\tLoss: 0.7758\t MAE: 0.6776\n",
      "Epoch  88\n",
      "  0 sec | Step 157\tLoss: 0.5085\t MAE: 0.5268\n",
      "  0 sec | Step  39\tLoss: 0.7758\t MAE: 0.6776\n",
      "Epoch  89\n",
      "  0 sec | Step 157\tLoss: 0.5085\t MAE: 0.5268\n",
      "  0 sec | Step  39\tLoss: 0.7758\t MAE: 0.6776\n",
      "Epoch  90\n",
      "  0 sec | Step 157\tLoss: 0.5087\t MAE: 0.5269\n",
      "  0 sec | Step  39\tLoss: 0.7758\t MAE: 0.6776\n",
      "Epoch  91\n",
      "  0 sec | Step 157\tLoss: 0.5084\t MAE: 0.5267\n",
      "  0 sec | Step  39\tLoss: 0.7758\t MAE: 0.6776\n",
      "Epoch  92\n",
      "  0 sec | Step 157\tLoss: 0.5084\t MAE: 0.5267\n",
      "  0 sec | Step  39\tLoss: 0.7758\t MAE: 0.6777\n",
      "Epoch  93\n",
      "  0 sec | Step 157\tLoss: 0.5087\t MAE: 0.5269\n",
      "  0 sec | Step  39\tLoss: 0.7759\t MAE: 0.6777\n",
      "Epoch  94\n",
      "  0 sec | Step 157\tLoss: 0.5085\t MAE: 0.5268\n",
      "  0 sec | Step  39\tLoss: 0.7759\t MAE: 0.6777\n",
      "Epoch  95\n",
      "  0 sec | Step 157\tLoss: 0.5086\t MAE: 0.5268\n",
      "  0 sec | Step  39\tLoss: 0.7759\t MAE: 0.6777\n",
      "Epoch  96\n",
      "  0 sec | Step 157\tLoss: 0.5088\t MAE: 0.5269\n",
      "  0 sec | Step  39\tLoss: 0.7759\t MAE: 0.6777\n",
      "Epoch  97\n",
      "  0 sec | Step 157\tLoss: 0.5084\t MAE: 0.5267\n",
      "  0 sec | Step  39\tLoss: 0.7759\t MAE: 0.6777\n",
      "Epoch  98\n",
      "  0 sec | Step 157\tLoss: 0.5088\t MAE: 0.5269\n",
      "  0 sec | Step  39\tLoss: 0.7759\t MAE: 0.6777\n",
      "Epoch  99\n",
      "  0 sec | Step 157\tLoss: 0.5082\t MAE: 0.5266\n",
      "  0 sec | Step  39\tLoss: 0.7759\t MAE: 0.6777\n",
      "Epoch 100\n",
      "  0 sec | Step 157\tLoss: 0.5085\t MAE: 0.5268\n",
      "  0 sec | Step  39\tLoss: 0.7759\t MAE: 0.6777\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "train_mean_losses = []\n",
    "valid_mean_losses = []\n",
    "train_maes = []\n",
    "valid_maes = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_mae.reset_states()\n",
    "    validation_mae.reset_states()\n",
    "    mean_train_loss = 0.0\n",
    "    mean_valid_loss = 0.0\n",
    "    start = time.time()\n",
    "    learning_rate = scheduler(epoch)\n",
    "    optimizer.learning_rate = learning_rate\n",
    "    print(f'Epoch {epoch + 1:>3}')\n",
    "\n",
    "    for step, (movie, user, y_true) in enumerate(train_ds):\n",
    "        loss = training_step(movie, user, y_true)\n",
    "        \n",
    "        mae = train_mae.result()\n",
    "        mean_train_loss = mean_train_loss + (1 / (step + 1)) * (loss - mean_train_loss)\n",
    "        end = time.time()\n",
    "        print(f'\\r{int(end - start):>3} sec | Step {step:>3}\\tLoss: {mean_train_loss:>2.4f}\\t MAE: {mae:>2.4f}', end='')\n",
    "    print()\n",
    "    train_mean_losses.append(mean_train_loss)\n",
    "    train_maes.append(mae)\n",
    "\n",
    "    for step, (movie, user, y_true) in enumerate(test_ds):\n",
    "        loss = validation_step(movie, user, y_true)\n",
    "\n",
    "        mae = validation_mae.result()\n",
    "        mean_valid_loss = mean_valid_loss + (1 / (step + 1)) * (loss - mean_valid_loss)\n",
    "        end = time.time()\n",
    "        print(f'\\r{int(end - start):>3} sec | Step {step:>3}\\tLoss: {mean_valid_loss:>2.4f}\\t MAE: {mae:>2.4f}', end='')\n",
    "    print()\n",
    "    valid_mean_losses.append(mean_valid_loss)\n",
    "    valid_maes.append(mae)\n",
    "\n",
    "history = {'loss':train_mean_losses, 'val_loss':valid_mean_losses, 'mae':train_maes, 'val_mae':valid_maes}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3ee758-c3a0-4b2b-ade1-be62c1390d3b",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e240438-bcf9-40a8-a452-9ddb73c66748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nine to Five (a.k.a. 9 to 5) (1980)\n",
      "4.495363\n",
      "\n",
      "Third Man, The (1949)\n",
      "4.423199\n",
      "\n",
      "Dracula (Bram Stoker's Dracula) (1992)\n",
      "4.409568\n",
      "\n",
      "Gravity (2013)\n",
      "4.4060683\n",
      "\n",
      "Braveheart (1995)\n",
      "4.3816185\n",
      "\n",
      "Pest, The (1997)\n",
      "4.3786983\n",
      "\n",
      "Three Colors: Blue (Trois couleurs: Bleu) (1993)\n",
      "4.376232\n",
      "\n",
      "Face/Off (1997)\n",
      "4.3742337\n",
      "\n",
      "Mallrats (1995)\n",
      "4.3741074\n",
      "\n",
      "Smokin' Aces (2006)\n",
      "4.372439\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unique_movies = np.unique(movies)\n",
    "test_users = (np.ones(unique_movies.shape[0]) * 249).astype('float32')\n",
    "test_movies = unique_movies.astype('float32')\n",
    "pred = model(test_movies, test_users, training=False)[:, 0]\n",
    "indx_pred = np.argsort(pred, axis=0)[::-1]\n",
    "\n",
    "for i in range(10):\n",
    "    print(titles.iloc[indx_pred[i]])\n",
    "    print(pred[indx_pred[i]].numpy() + mu)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0da48360-ae97-4334-80de-d3f5fe58d64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted 4.2 and the true rating was 4.0\n",
      "Predicted 3.7 and the true rating was 4.0\n",
      "Predicted 3.6 and the true rating was 3.0\n",
      "Predicted 3.4 and the true rating was 5.0\n",
      "Predicted 3.4 and the true rating was 3.0\n",
      "Predicted 4.2 and the true rating was 2.0\n",
      "Predicted 3.2 and the true rating was 4.5\n",
      "Predicted 3.1 and the true rating was 2.0\n",
      "Predicted 3.3 and the true rating was 5.0\n",
      "Predicted 3.8 and the true rating was 4.0\n",
      "Predicted 3.5 and the true rating was 3.0\n",
      "Predicted 3.2 and the true rating was 2.0\n",
      "Predicted 3.3 and the true rating was 2.5\n",
      "Predicted 3.8 and the true rating was 4.0\n",
      "Predicted 3.6 and the true rating was 3.5\n",
      "Predicted 4.1 and the true rating was 2.0\n",
      "Predicted 4.0 and the true rating was 2.5\n",
      "Predicted 3.9 and the true rating was 3.5\n",
      "Predicted 3.3 and the true rating was 4.5\n",
      "Predicted 3.7 and the true rating was 4.5\n",
      "Predicted 3.5 and the true rating was 3.5\n",
      "Predicted 3.6 and the true rating was 3.0\n",
      "Predicted 3.5 and the true rating was 2.0\n",
      "Predicted 3.2 and the true rating was 3.5\n",
      "Predicted 3.8 and the true rating was 3.0\n",
      "Predicted 3.6 and the true rating was 3.5\n",
      "Predicted 3.4 and the true rating was 5.0\n",
      "Predicted 3.8 and the true rating was 5.0\n",
      "Predicted 4.1 and the true rating was 2.0\n",
      "Predicted 3.1 and the true rating was 5.0\n",
      "Predicted 3.4 and the true rating was 3.0\n",
      "Predicted 4.2 and the true rating was 4.0\n",
      "Predicted 3.6 and the true rating was 3.5\n",
      "Predicted 3.7 and the true rating was 2.0\n",
      "Predicted 3.3 and the true rating was 2.5\n",
      "Predicted 3.6 and the true rating was 5.0\n",
      "Predicted 3.7 and the true rating was 3.5\n",
      "Predicted 3.9 and the true rating was 4.0\n",
      "Predicted 3.3 and the true rating was 3.0\n",
      "Predicted 3.1 and the true rating was 3.0\n",
      "Predicted 3.2 and the true rating was 5.0\n",
      "Predicted 3.4 and the true rating was 3.0\n",
      "Predicted 3.4 and the true rating was 1.0\n",
      "Predicted 4.3 and the true rating was 4.0\n",
      "Predicted 3.2 and the true rating was 3.5\n",
      "Predicted 3.1 and the true rating was 4.0\n",
      "Predicted 4.3 and the true rating was 3.0\n",
      "Predicted 3.9 and the true rating was 5.0\n",
      "Predicted 3.6 and the true rating was 3.0\n",
      "Predicted 3.0 and the true rating was 4.5\n",
      "Predicted 4.2 and the true rating was 4.5\n",
      "Predicted 3.3 and the true rating was 4.5\n",
      "Predicted 4.0 and the true rating was 4.5\n",
      "Predicted 3.3 and the true rating was 4.0\n",
      "Predicted 3.9 and the true rating was 4.0\n",
      "Predicted 3.5 and the true rating was 4.0\n",
      "Predicted 3.1 and the true rating was 2.0\n",
      "Predicted 3.1 and the true rating was 3.5\n",
      "Predicted 2.9 and the true rating was 4.0\n",
      "Predicted 3.0 and the true rating was 3.0\n",
      "Predicted 3.1 and the true rating was 2.0\n",
      "Predicted 3.9 and the true rating was 3.5\n",
      "Predicted 3.9 and the true rating was 3.0\n",
      "Predicted 2.9 and the true rating was 2.5\n",
      "Predicted 3.5 and the true rating was 4.5\n",
      "Predicted 3.6 and the true rating was 4.0\n",
      "Predicted 4.0 and the true rating was 4.0\n",
      "Predicted 3.1 and the true rating was 2.0\n",
      "Predicted 3.5 and the true rating was 2.5\n",
      "Predicted 4.2 and the true rating was 1.5\n",
      "Predicted 3.6 and the true rating was 3.0\n",
      "Predicted 3.1 and the true rating was 4.0\n",
      "Predicted 4.3 and the true rating was 4.5\n",
      "Predicted 3.0 and the true rating was 3.0\n",
      "Predicted 4.1 and the true rating was 3.0\n",
      "Predicted 4.1 and the true rating was 5.0\n",
      "Predicted 4.0 and the true rating was 3.5\n",
      "Predicted 3.8 and the true rating was 5.0\n",
      "Predicted 3.5 and the true rating was 3.5\n",
      "Predicted 3.7 and the true rating was 4.0\n",
      "Predicted 3.4 and the true rating was 3.0\n",
      "Predicted 3.2 and the true rating was 4.0\n",
      "Predicted 3.5 and the true rating was 3.0\n",
      "Predicted 3.9 and the true rating was 3.0\n",
      "Predicted 3.2 and the true rating was 3.5\n",
      "Predicted 4.1 and the true rating was 4.0\n",
      "Predicted 3.9 and the true rating was 4.0\n",
      "Predicted 3.8 and the true rating was 2.5\n",
      "Predicted 3.3 and the true rating was 4.0\n",
      "Predicted 3.8 and the true rating was 2.5\n",
      "Predicted 3.2 and the true rating was 4.0\n",
      "Predicted 3.7 and the true rating was 3.0\n",
      "Predicted 3.7 and the true rating was 3.0\n",
      "Predicted 3.4 and the true rating was 5.0\n",
      "Predicted 3.9 and the true rating was 2.5\n",
      "Predicted 3.7 and the true rating was 4.0\n",
      "Predicted 4.2 and the true rating was 2.0\n",
      "Predicted 4.2 and the true rating was 4.5\n",
      "Predicted 4.0 and the true rating was 5.0\n",
      "Predicted 3.6 and the true rating was 3.5\n"
     ]
    }
   ],
   "source": [
    "max_loss = 0\n",
    "y_pred = model(test_movies, test_users, training=False)\n",
    "for i in range(100):\n",
    "    y_hat = y_pred[i].numpy()\n",
    "    y_true = test_ratings[i]\n",
    "    print(f'Predicted {y_hat[0] + mu:>2.2} and the true rating was {y_true + mu:>2.2}')\n",
    "    max_loss = max(max_loss, np.abs(y_hat - y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6908027-b22a-4bd1-a82e-dc6a3b1fa855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.6941442], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "488817c1-ee90-4572-a5b6-61c3fad4619f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/collaborative_filtering_v1.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./models/collaborative_filtering_v1.tf/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('./models/collaborative_filtering_v1.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1be037-d5cb-4e9c-ac55-57131fb25a7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
